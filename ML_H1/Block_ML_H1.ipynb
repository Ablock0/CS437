{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Block_ML.H1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ablock0/CS437/blob/master/Block_ML_H1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b71Bo5s_-axc"
      },
      "source": [
        "# **Homework Assignment #1**\n",
        "\n",
        "Assigned: January 13, 2020\n",
        "\n",
        "Due: January 27, 2020\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of four questions that require a short answer and one that requires you to generate some Python code. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(8 points) Imagine that you are given the task of building a system to distinguish junk email from important messages. Consider aspects of junk email that let us know that it is junk and think about how a computer could use such information to automatically label messages as \"junk\" or \"real\".\n",
        "\n",
        "For this question, generate at least 10 features that a computer could automatically extract from messages and use to determine if a message is junk or real. For each feature, briefly describe how a computer would extract the feature and why it is useful for this task.\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(5 points) Imagine that you are given the task of building a system to distinguish fraudulent credit card transactions from legitimate credit card transactions. For this question, generate at least 10 features that a computer could automatically extract from transaction information and determine if the transaction is \"fraudulent\" or \"legitimate\". You do not need to provide explanations for the features in this problem.\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(8 points) Does the error of a hypothesis measured over a training set provide a pessimistically-biased estimate or an optimistically-biased estimate of the true error? Explain your answer.\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(25 points) Given the following data set containing three attributes and one class, construct a decision tree, then use this tree to determine the class value (Yes/No) of Stolen for a red domestic SUV.\n",
        "\n",
        "ID | Color | Type | Origin | Stolen\n",
        "--- | --- | --- | --- | ---\n",
        "1 | Red        |  Sports  | Domestic        | Yes\n",
        "2 | Red        |  Sports  | Domestic        | No\n",
        "3 | Red        |  Sports  | Domestic        | Yes\n",
        "4 | Yellow   |    Sports  | Domestic        | No \n",
        "5 | Yellow   |    Sports |  Imported        | Yes\n",
        "6 | Yellow   |    SUV     | Imported        | No\n",
        "7 | Yellow   |    SUV     | Imported        | Yes\n",
        "8 | Yellow   |    SUV     | Domestic        | No\n",
        "9 | Red        |  SUV     | Imported        | No\n",
        "10 | Red        |  Sports  | Imported        | Yes\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#5.\n",
        "\n",
        "(80 points) In this problem you are asked to write a Python program using Google Colab to construct a decision tree using the data from problem #4. Your program should use the measures of entropy and gain discussed in class to select features, should resolve ties randomly, and should label leaf nodes using majority vote. You may hard code the dataset or (more elegantly) design the program to read the data from a file. Finally, your program should be able to use the learned decision tree to output a label for the new data point introduced in problem #4.  *Note that all of the code you write needs to be entirely your own, not copied from another existing program or using existing libraries that perform the specified functionality.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MybXjx2MtGDx",
        "colab_type": "text"
      },
      "source": [
        "# ANSWERS\n",
        "# 1\n",
        "- Subject length - We can write a program to test the length of the subject line, if it is very short then it is more likely to be spam\n",
        "- Subject contents - Similarly, we can grab the subject line and analyze it for any key words. If it just says random things like HELLO in all caps, like the example from class, or even something like ACT NOW, then it is most likely spam\n",
        "- Punctuation mistakes - If we see punctuation mistakes such as a space before and after a period or question mark, then we know this email wasnt properly written\n",
        "- Spelling mistakes - If we allow the machine access to the full dictionary of words and it comes accross a word that is not spelled correctly then it will not recognize it, therefor it can mark the email as potential spam easily\n",
        "- No proper line breaks / spacing - We can have the algorithm run to check for line breaks or if the email is just one giant paragraph. Having no spacing is usually less professional and will most likely be spam\n",
        "- Email address - We can have the algorithm analyze the email address of the sender, if it looks odd or is a random address, it may be suspicious. We could even go so far as to give the machine access to a database of known spam emails from the web and it can compare it to that list.\n",
        "- Signature at the end - Signatures at the end of emails are usually very well constructed, so if there is no signature or it is poorly written, we can have our machine learning algorithm flag it as potential spam.\n",
        "- Domain - The domain of the email address can also tell a lot. We can again have our algorithm compare this to a list of knowm spam accounts on the web, or we can have it analyze that section of the email address to see if it is actually from the company or account of the sender.\n",
        "- Capitalization mistakes - We can train our algorithm to look for proper capitalization after the occurance of specific punctuations or for words like \"I\". \n",
        "- Collection of key words - Our algorithm can maybe be taught to look through the email and count the number of occuring key words in each email. We could even take this a step further and compare this number to the subject line to see if they make sense or if it is just spam.\n",
        "---\n",
        "#2\n",
        "- Number of transactions made\n",
        "- Location\n",
        "- Amount\n",
        "- Tip percentage if there was a tip\n",
        "- Have they shopped here before\n",
        "- Time of purchase\n",
        "- Item bought\n",
        "- Date\n",
        "- Online vs in store purchase\n",
        "- Did it overdraft the card / account\n",
        "---\n",
        "#3\n",
        "\n",
        "The error of a hypothesis over a training set provides an optimistically-biased estimate of the true error because it is biased and usually smaller than the true error.\n",
        "\n",
        "---\n",
        "#4\n",
        "Refer to the picture in the github repository\n",
        "\n",
        "---\n",
        "#5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX-Ar_lAVyh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#all import variables done initially\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5HMd_2XWIQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read in the data using pandas\n",
        "#addecd the names because I was having a ton of trouble with splitting X and y\n",
        "data = pd.read_csv(r'/content/Data_File.csv', sep = ',', skiprows = 1, header = None, names =['Color','Type','Origin','Stolen'])\n",
        "\n",
        "#this was a nightmare, but apparently you can just split vai the column names\n",
        "X = data[['Color', 'Type', 'Origin']]\n",
        "y = data['Stolen']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEBCLUOUaCdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "2248f46c-919c-4322-8ace-14ede3a7077f"
      },
      "source": [
        "#output X result to check\n",
        "X.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Color</th>\n",
              "      <th>Type</th>\n",
              "      <th>Origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Red</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Domestic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Red</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Domestic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Red</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Domestic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Yellow</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Domestic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yellow</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Imported</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Color    Type    Origin\n",
              "0     Red  Sports  Domestic\n",
              "1     Red  Sports  Domestic\n",
              "2     Red  Sports  Domestic\n",
              "3  Yellow  Sports  Domestic\n",
              "4  Yellow  Sports  Imported"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBQ_IHWBXXzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node:\n",
        "    #defining initial build of NOde\n",
        "    def __init__(self, x, y, arr, low=5):\n",
        "        #initalize all values\n",
        "        self.x = x \n",
        "        self.y = y\n",
        "        self.arr = arr \n",
        "        self.num_cols = x.shape[1]\n",
        "        self.num_rows = len(arr)\n",
        "        self.val = .95\n",
        "\n",
        "        #FIX THIS\n",
        "        #the min_leaf and score will be used later for comparisons\n",
        "        self.low = low\n",
        "        self.score = float('inf')\n",
        "\n",
        "        #write a split function below\n",
        "        self.main_split()\n",
        "        \n",
        "    # checking to see if we are at a leaf node, will be used in split function    \n",
        "    @property # need property to allow self call below\n",
        "    def is_leaf(self): return self.score == float('inf')\n",
        "\n",
        "    # splitting between rows and columns, used in split function\n",
        "    @property # still need this for self call\n",
        "    def split_col(self): return self.x.values[self.arr,self.col] \n",
        "\n",
        "    def main_split(self):\n",
        "        #call a helper split function\n",
        "        for temp in range(self.num_cols): self.helper_split(temp)\n",
        "        \n",
        "        #if we are at leaf node we are good to go\n",
        "        if self.is_leaf: return\n",
        "\n",
        "        #else: not a leaf node, need to creat children\n",
        "        x = self.split_col\n",
        "        right = np.nonzero(x > self.split)[0]\n",
        "        left = np.nonzero(x <= self.split)[0]\n",
        "        #creating children by creating left and right which are two more nodes with subsets of the overall data\n",
        "        self.left = Node(self.x, self.y, self.arr[left], self.low)\n",
        "        self.right = Node(self.x, self.y, self.arr[right], self.low)\n",
        "        \n",
        "    def helper_split(self, col):\n",
        "        # collect values\n",
        "        x = self.x.values[self.arr, col]\n",
        "        # iterate through each row\n",
        "        for temp_2 in range(self.num_rows):\n",
        "            # set each left and right subtree value according to curr row\n",
        "            left = x <= x[temp_2]\n",
        "            right = x > x[temp_2]\n",
        "            if right.sum() < self.low or left.sum() < self.low: continue\n",
        "\n",
        "            # need to update the value of current node\n",
        "            val_curr = self.helper_score(left, right) # using help function\n",
        "            if val_curr < self.score: #update score with lower val and update column\n",
        "                self.col = col\n",
        "                self.score = val_curr\n",
        "                self.split = x[temp_2] #split on new value of row\n",
        "                                \n",
        "    def helper_score(self, left, right):\n",
        "        # helper function for score calculation\n",
        "        y = self.y[self.arr] # collect y values\n",
        "        left_deviation = y[left].std()\n",
        "        right_deviation = y[right].std()\n",
        "\n",
        "        #calculate the new score and return value\n",
        "        ret_score = left_deviation * left.sum() + right_deviation * right.sum()\n",
        "        return ret_score\n",
        "              \n",
        "    def predict(self, x):\n",
        "        #prediction function\n",
        "        ret_array = np.array([self.helper_predict(pred) for pred in x])\n",
        "        return ret_array\n",
        "\n",
        "    def helper_predict(self, pred):\n",
        "        #helper function for prediction\n",
        "        if self.is_leaf: return self.val #get value to fill array for prediction\n",
        "        node = self.left if pred[self.col] <= self.split else self.right\n",
        "        return node.helper_predict(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjNXAB4-XnT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecisionTree:\n",
        "  #Tree class that will call our node class to build each node for\n",
        "  def BuildTree(self, X, y, low = 5):\n",
        "    #func to create our tree\n",
        "    self.my_decision_tree = Node(X, y, np.array(np.arange(len(y))), low) #call our Node class above\n",
        "    return self\n",
        "  \n",
        "  def GetPrediction(self, X):\n",
        "    # call Node.predict func from above class\n",
        "    return self.my_decision_tree.predict(X.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ED98Eo3Xwpn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9594039-7637-4bca-e091-3eb47b7e8197"
      },
      "source": [
        "Tree = DecisionTree().BuildTree(X, y) #build the tree\n",
        "outcome = Tree.GetPrediction(X) # get prediction array to use below\n",
        "\n",
        "# print out the accuracy of our decision tree algorithm\n",
        "# I am multiplying by -1 because the model comes out as negative but the pos or neg value doesnt matter\n",
        "# so the pos value looks better as an outcome\n",
        "(-1)*metrics.r2_score(y, outcome)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.81"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}