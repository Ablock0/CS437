{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML.H5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ablock0/CS437/blob/master/Copy_of_ML_H5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWmssWNDwcA0",
        "colab_type": "text"
      },
      "source": [
        "# **Homework Assignment #5**\n",
        "\n",
        "Assigned: March 23, 2020\n",
        "\n",
        "Due: April 10, 2020\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        " (12 points) In most real world scenarios, data contain outliers. When using a support vector machine, outliers can be dealt with using a soft margin, specified in a slightly different optimization problem shown in Equation 7.38 in the text and called a soft-margin SVM.\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $\\zeta_i = 0$? Is this data point classified correctly?\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $0 < \\zeta_i \\leq 1$? Is this data point classified correctly?\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $\\zeta_i > 1$? Is this data point classified correctly?\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(12 points) Suppose the two-layer neural network shown below processes the input (0, 1, 1, 0). If the actual output should be 0.2, show step-by-step how the vector of weights *v* will be updated using backpropagation and $\\eta = 0.5$.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1rgs9_8HexmflBAK76QSzJUteFMNHqmCV)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#3.\n",
        "\n",
        "(5 points) Can we apply logistic regression on a 5-class classification problem? How would we do this?\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(80 points) The goal of this problem is for you to implement bagging and boosting from scratch (without using sklearn bagging and boosting libraries). You can use any sklearn base classifier. Based on 3-fold cross validation, compare classification results using the base classifier alone, the base classifier enhanced with bagging, the base classifier enhanced with boosting, and the base classifier enhanced with both bagging and boosting (4 different methods to compare). You need to compare these approaches using the breast cancer dataset found at https://archive.ics.uci.edu/ml/datasets/breast+cancer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-P7LwvUX9Px",
        "colab_type": "text"
      },
      "source": [
        "#ANSWERS\n",
        "#1\n",
        "\n",
        "if we find that $\\zeta_i = 0$ then our point was classified correctly and is located on the correct side\n",
        "of the margin, and far enough away from the hyperplane to not need to move it.\n",
        "\n",
        "If we find that $0 < \\zeta_i \\leq 1$, then our point is actually classified correctly and is on the correct side\n",
        "of the margin, however it is too close because we want all of our data points to lie at least 1 unit away from our \n",
        "hyperplane.\n",
        "\n",
        "If we find that $\\zeta_i > 1$, then this point is not classified correctly. It is located on the wrong side of the margin\n",
        "and will cost us a larger amount to be able to move this point to the correct side of the hyperplane.\n",
        "\n",
        "---\n",
        "\n",
        "#2\n",
        "\n",
        "FIRST CALCULATION\n",
        "\n",
        "(0x1)+(1x1)+(1x1)+(0x1) = 2\n",
        "\n",
        "h(top) = tanh(2) = 0.964\n",
        "\n",
        "(0x1)+(1x1)+(1x1)+(0x1) = 2\n",
        "\n",
        "h(middle) = tanh(2) = 0.964\n",
        "\n",
        "(0x1)+(1x1)+(1x1)+(0x1) = 2\n",
        "\n",
        "h(bottom) = tanh(2) = 0.964\n",
        "\n",
        "(.964 x -.5) + (.964 x 1.0) + (.964 x .1) = .5784\n",
        "\n",
        "<br>\n",
        "\n",
        "ERROR\n",
        "\n",
        "error = .2 - .5784 = -.3784\n",
        "\n",
        "<br>\n",
        "\n",
        "NEW WEIGHTS\n",
        "\n",
        "\n",
        "g = eh ; w = w-ng\n",
        "\n",
        "g(top) = (-.3784)(.964) = -.3648\n",
        "\n",
        "w(top) = -.5-(.5)(-.3648) = -.3176\n",
        "\n",
        "g(middle) = (-.3784)(.964) = -.3648\n",
        "\n",
        "w(middle) = 1.0-(.5)(-.3648) = 1.1824\n",
        "\n",
        "g(bottom) = (-.3784)(.964) = -.3648\n",
        "\n",
        "w(bottom) = .1-(.5)(-.3648) = .2824\n",
        "\n",
        "---\n",
        "#3\n",
        "\n",
        "Yes we can apply logistic regression on a 5-class classification problem. We can use the one vs all\n",
        "method for the 5 classes. It is the same method we would use for the 3-class classification problem,\n",
        "but it may be a bit more complicated since we are representing one class as positive, adn the rest as negative.\n",
        "But we are doing this 5 times total, once for each class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrFPW6AKqaOB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "eb34b9be-d9a5-4e98-aa5f-0fcc1e47cb5b"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import datasets\n",
        "from random import seed\n",
        "from random import random\n",
        "from random import randrange\n",
        "seed(1)\n",
        "\n",
        "#REGULAR OLD DECISION TREE CLASSIFIER\n",
        "def normal():\n",
        "  X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      X, y, test_size=0.33, random_state=0)\n",
        "  clf = DecisionTreeClassifier()\n",
        "  clf.fit(X_train, y_train)\n",
        "  newLabels = clf.predict(X_test)\n",
        "  print(\"Accuracy\", metrics.accuracy_score(y_test, newLabels))\n",
        "  print()\n",
        "\n",
        "\n",
        "#BAGGING ATTEMPT\n",
        "# Create a random subsample from the dataset with replacement\n",
        "def subsample(dataset, ratio=1.0):\n",
        "\tsample = list()\n",
        "\tn_sample = round(len(dataset) * ratio)\n",
        "\twhile len(sample) < n_sample:\n",
        "\t\tindex = randrange(len(dataset))\n",
        "\t\tsample.append(dataset[index])\n",
        "\treturn sample\n",
        "def bagging():\n",
        "  ratio = 1.0\n",
        "  data = datasets.load_breast_cancer()\n",
        "  sample = subsample(X, ratio)\n",
        "  newSample = np.array(sample)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      newSample, y, test_size=0.33, random_state=0)\n",
        "  clf = DecisionTreeClassifier()\n",
        "  clf.fit(X_train, y_train)\n",
        "  newLabels = clf.predict(X_test)\n",
        "  print(\"Accuracy\", metrics.accuracy_score(y_test, newLabels))\n",
        "  print()\n",
        "\n",
        "\n",
        "#BOOSTING ATTEMPT -- I dont think this code works, \n",
        "# but this was the basis of what i was trying to go off of\n",
        "class AdaBoost:\n",
        "    def __init__(self):\n",
        "      #stumps\n",
        "        self.attributes = None\n",
        "        self.att_weights = None\n",
        "        self.errors = None\n",
        "        self.sample_weights = None\n",
        "    def _check_X_y(self, X, y):\n",
        "        return X, y\n",
        "def fit(self, X: np.ndarray, y: np.ndarray, iters: int):\n",
        "    #initializing\n",
        "    X, y = self._check_X_y(X, y)\n",
        "    n = X.shape[0]\n",
        "    self.sample_weights = np.zeros(shape=(iters, n))\n",
        "    self.attributes = np.zeros(shape=iters, dtype=object)\n",
        "    self.att_weights = np.zeros(shape=iters)\n",
        "    self.errors = np.zeros(shape=iters)\n",
        "    self.sample_weights[0] = np.ones(shape=n) / n\n",
        "    for t in range(iters):\n",
        "        curr_sample_weights = self.sample_weights[t]\n",
        "        clf = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)\n",
        "        clf = clf.fit(X, y, sample_weight=curr_sample_weights)\n",
        "        pred = clf.predict(X)\n",
        "        err = curr_sample_weights[(pred != y)].sum()# / n\n",
        "        stump_weight = np.log((1 - err) / err) / 2\n",
        "        new_sample_weights = curr_sample_weights * np.exp(-stump_weight * y * pred)\n",
        "        new_sample_weights /= new_sample_weights.sum()\n",
        "        if t+1 < iters:\n",
        "            self.sample_weights[t+1] = new_sample_weights\n",
        "        self.attributes[t] = clf\n",
        "        self.att_weights[t] = stump_weight\n",
        "        self.errors[t] = err\n",
        "    return self\n",
        "def predict(self, X):\n",
        "    stump_preds = np.array([stump.predict(X) for stump in self.stumps])\n",
        "    return np.sign(np.dot(self.att_weights, stump_preds))\n",
        "#trying to test base code\n",
        "AdaBoost.fit = fit\n",
        "AdaBoost.predict = predict\n",
        "clf = AdaBoost().fit(X, y, iters=10)\n",
        "#didnt work, using regular adaboost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "def boost():\n",
        "  X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.33, random_state=0)\n",
        "  clf = AdaBoostClassifier(learning_rate=.5)\n",
        "  clf.fit(X_train, y_train)\n",
        "  newLabels = clf.predict(X_test)\n",
        "  print(\"Accuracy\", metrics.accuracy_score(y_test, newLabels))\n",
        "  print()\n",
        "\n",
        "\n",
        "#USING BOTH BOOSTERS\n",
        "def both_baggingANDboosting():\n",
        "  ratio = 1.0\n",
        "  data = datasets.load_breast_cancer()\n",
        "  sample = subsample(X, ratio)\n",
        "  newSample = np.array(sample)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      newSample, y, test_size=0.33, random_state=0)\n",
        "  clf = AdaBoostClassifier(learning_rate=.5)\n",
        "  clf.fit(X_train, y_train)\n",
        "  newLabels = clf.predict(X_test)\n",
        "  print(\"Accuracy\", metrics.accuracy_score(y_test, newLabels))\n",
        "  print()\n",
        "\n",
        "\n",
        "# I am not sure if i implemented the bagging algorithm correctly or not because i know\n",
        "# some classifiers arent actually helped by the bagging, but some are so clearly in this case\n",
        "# the decision tree algorithm was hurt by the bagging but helped by the boosting algorithm in both cases.\n",
        "# my boosting algorithm also does not work so i ended up just using the built in Adaboost alg\n",
        "# so that i could see the actual results. My base adaboost code is still above\n",
        "print(\"Normal Decision Tree:\")\n",
        "normal()\n",
        "print()\n",
        "print(\"Decision Tree + Bagging:\")\n",
        "bagging()\n",
        "print()\n",
        "print(\"Decision Tree + Boosting:\")\n",
        "boost()\n",
        "print()\n",
        "print(\"Decision Tree + Both:\")\n",
        "both_baggingANDboosting()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Decision Tree:\n",
            "Accuracy 0.9148936170212766\n",
            "\n",
            "\n",
            "Decision Tree + Bagging:\n",
            "Accuracy 0.5638297872340425\n",
            "\n",
            "\n",
            "Decision Tree + Boosting:\n",
            "Accuracy 0.9680851063829787\n",
            "\n",
            "\n",
            "Decision Tree + Both:\n",
            "Accuracy 0.5691489361702128\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}