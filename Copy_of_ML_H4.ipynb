{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML.H4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ablock0/CS437/blob/master/Copy_of_ML_H4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7EVsHcpkNfH",
        "colab_type": "text"
      },
      "source": [
        "# **Homework Assignment #4**\n",
        "\n",
        "Assigned: February 28, 2020\n",
        "\n",
        "Due: March 23, 2020\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        " (10 points) Consider the subset of (x,y) points shown below. These are actually a subset of the data points found in the sklearn diabetes dataset.\n",
        "\n",
        "x | y\n",
        "--- | ---\n",
        " 0.08 | 233\n",
        "-0.04 | 91\n",
        " 0.01 | 111\n",
        "-0.04 | 152\n",
        "-0.03 | 120\n",
        " 0.01 | 67\n",
        " 0.09 | 310\n",
        "-0.03 | 94\n",
        "-0.06 | 183\n",
        "-0.03 | 66\n",
        " 0.06 | 173\n",
        "-0.06 | 72\n",
        " 0.00 | 49\n",
        "-0.02 | 64\n",
        "-0.07 | 48\n",
        "\n",
        "\n",
        "For a candidate linear regressor with parameters $\\Theta_0 = 7.42968$ and $\\Theta_1 = -0.002945$, calculate the mean squared error with respect to the data points and perform one iteration of gradient descent , assuming $\\alpha = 0.01$. Show all of your work.\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(10 points) Using the unified loss function graph below, plot the functions for the 0/1 loss, hinge loss, logistic loss, exponential loss, and squared loss functions we discussed in class. For each loss function, explain what is distinct about the loss function and in what context it might be used.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1kZISYnrHr0GJ0WcRnu0-e6fbRB1wZADz)\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(100 points) Implement Naive Rank Train with Naive Rank Test, or RankTrain with RankTest, or both, for movie ratings from the MovieLens education ml-latest-small (size: 1MB) dataset found at https://grouplens.org/datasets/movielens/. To augment this dataset, we have collected the plot summary (plots-imdb.csv, https://drive.google.com/open?id=1lupQzmOlOHxWWTryHWmyxEDYT3JKoafP) and rating (ratings-imdb.csv, https://drive.google.com/open?id=1Vs2q6e36Ea32pvbuMLy11fiUrqLnqiyK) available from IMDb for the movies in the MovieLens dataset. Evaluate each ranking using the Kemeny distance measure and focus only on the top 50 results, where the IMDb ratings are considered ground truth. For the Kemeny distance you will need to modify the original distance definition to use the average difference between the ranks of each movie. Each pair of movies should be represented as a feature vector containing their release years, and bag of words (Count Vectorizer) for each of their genres, titles and plots. Evaluate the results with and without each of the two text mining tools we discussed in class: stop word filtering, and word frequency-inverse document frequency instead of word counts.\n",
        "\n",
        "Note: Using all 1000 movies in the provided datasets will take over an hour to finish. To save time, you can provided results based on the first 200 movies in the list (which took 7 minutes for the solution to run). However, if you have time I encourage you to try all 1000 movies because you will see more titles that you will likely recognize!\n",
        "\n",
        "---\n",
        "\n",
        "#Project Proposal.\n",
        "\n",
        "(10 points of your project grade)\n",
        "\n",
        "Write a brief project proposal. This should include the goal of the project, the data you will use, likely data preparation steps, proposed machine learning methods, novelty of the technique beyond what was discussed in class (if applicable), and evaluation steps. The proposal should include a list of team members with assigned roles as well (if applicable).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PwI_4cdcH-h",
        "colab_type": "text"
      },
      "source": [
        "#ANSWERS\n",
        "\n",
        "---\n",
        "#1\n",
        "see picture of Q1 parts 1 and 2 in github\n",
        "\n",
        "---\n",
        "#2\n",
        "see picture of Q2 in github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opzt2eoA8QX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I know that this code doesnt work and the integrated class code is not working with the other code but I basically got \n",
        "# to the point where i wasnt able to figure out how to correctly integrate the vectors of the movies so i decided to put all\n",
        "# the code that i was told to use in even though it isnt integrated properly. I didn't know how to progress\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction import stop_words\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "movies = np.loadtxt(fname='/content/movies.csv', delimiter=',')\n",
        "rankings = np.loadtxt(fname='/content/rankings.csv', delimeter = ',')\n",
        "\n",
        "\n",
        "# Rank functions from book, i didnt get to this part so they arent integrated / working\n",
        "def RankTrain (Data, BinaryTrain):\n",
        "  for i in range(i, N):\n",
        "    for all i,j = 1 to M and i != j:\n",
        "      if (i >= j):\n",
        "        #no change they are in the right order\n",
        "      else:\n",
        "        # j ranking is higher so need to change the order\n",
        "        temp = Data[i]\n",
        "        Data[i] = Data[j]\n",
        "        Data[j] = temp\n",
        "  return BinaryTrain(Data)\n",
        "def RankTest (Alg, x):\n",
        "  for all i,j = 1 to M and i != j:\n",
        "    y = Alg(x)\n",
        "    score_i += y\n",
        "    score_j += y\n",
        "  return ArgSort(score)\n",
        "\n",
        "\n",
        "# categories = ['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', \\\n",
        "#               'rec.sport.hockey', 'sci.med', 'sci.space', 'sci.electronics', \\\n",
        "#               'comp.graphics', 'comp.windows.x', 'talk.religion.misc', \\\n",
        "#               'talk.politics.mideast', 'talk.politics.misc']\n",
        "# newsgroups = fetch_20newsgroups(categories=categories)\n",
        "\n",
        "# not sure how to convert my rankings data into the same form as the newsgroup data is above here\n",
        "\n",
        "Vector = CountVectorizer()\n",
        "Tokens = TreebankWordTokenizer()\n",
        "# a bunch of set_params like we did in class code\n",
        "Vector.set_params(tokenizer=Tokens.tokenize)\n",
        "Vector.set_params(stop_words'english')\n",
        "#print(stop_words.ENGLISH_STOP_WORDS)\n",
        "Vector.set_params(ngram_range(1,2))\n",
        "Vector.set_params(max_df=0.5)\n",
        "Vector.set_params(min_df=2)\n",
        "Count_x = Vector.fit_transform(rankings.data) # newsgroups.data in class notes, i dont know what to put here\n",
        "Trans = TfidfTransformer()\n",
        "X_Trans = Trans.fit_transform(Count_x)\n",
        "clf = MultinomialNB().fit(X_Trans, rankings.target) # again, here is newsgroup.target, not sure how to convert\n",
        "scores = cross_val_score(clf, X_Trans, rankings.target, cv=3, scoring='') #newsgroup.target\n",
        "X = X_Trans\n",
        "y = rankings.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "#print statements from class here\n",
        "print('Confusion matrix:')\n",
        "pprint.pprint(newsgroups.target_names, width=200)\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}