{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ablock0/CS437/blob/master/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhfRa8wkJWba",
        "colab_type": "text"
      },
      "source": [
        "# **Homework Assignment #3**\n",
        "\n",
        "Assigned: February 12, 2020\n",
        "\n",
        "Due: February 28, 2020\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(5 points) Consider a classification model that separates email into two categories: \"junk\" or \"legitimate\". If you raise the classification threshold, what will happen to precision?  What will happen to recall? Briefly explain your answer.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(5 points) Which of classifiers described below (C1 or C2) seems to be doing a better job? Briefly explain your answer.\n",
        "\n",
        "C1: In the game of roulette, a ball is dropped on a spinning wheel and eventually lands in one of 38 slots. Using visual features (the spin of the ball, the position of the wheel when the ball was dropped, the height of the ball over the wheel), classifier C1 predicts the slot that the ball will land in with an accuracy of 4%. \n",
        "\n",
        "C2: The deadly but curable \"Dragon Pox\" disease afflicts 0.01% of the population. Classifier C2 uses symptoms as features and predicts this affliction with an accuracy of 99.99%.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(5 points) Recall the algorithm that was given in the text and in class for testing a ranking algorithm.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1VR9z8ap6M5WUBczWs3yFe2k2dQG08yfG)\n",
        "\n",
        "What is the runtime of this algorithm, expressed as a function of the number of objects to rank?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#4.\n",
        "\n",
        "(20 points) The table below shows the f-measures that resulted from 10-fold cross validation applied to classifiers A and B. Use a paired t-test and the t-table found at https://www.medcalc.org/manual/t-distribution.php\n",
        "to determine if the difference in performance between A and B is statistically significant. Show all of your work.\n",
        "\n",
        "$FMeasure_A$ | $FMeasure_B$\n",
        "--- | ---\n",
        ".40 | .45\n",
        ".41 | .46\n",
        ".42 | .47\n",
        ".55 | .65\n",
        ".55 | .65\n",
        ".70 | .62\n",
        ".73 | .83\n",
        ".89 | .90\n",
        ".53 | .54\n",
        ".88 | .82\n",
        ".71 | .70\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#5\n",
        "\n",
        "(80 points) For this programming task you will train a classifier to recognize human activities based on features extracted from wearable sensor data. The data collection is described at https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones. We will only be working with a portion of the data found at http://eecs.wsu.edu/~cook/ml/alldata.csv. The data at this site represents two activities: Walking upstairs (class value=0) and Walking downstairs (class value=1). When needed, you can consider the Walking upstairs class as the positive class and the Walking downstairs class as the negative class. The file is in comma-separated format. Each line represents a single data point. The data point is described by a set of comma-separated features, where the last feature on the line indicates the class label.\n",
        "\n",
        "First, write code to calculate accuracy and a macro f-measure using 3-fold cross validation for two classifiers: a majority classifier and a decision tree classifier. You can use the sklearn libraries for the classifiers but write your own code to perform cross validation and calculation of the performance measures.\n",
        "\n",
        "Second, provide your observations on the results. Why do the two performance measures provide such different results? Why do the two classifiers perform so differently on this task?\n",
        "\n",
        "Third, write code to generate and plot an ROC curve (containing at least 10 data points). Generate two ROC curves, one based on a decision tree classifier with a depth bound of 2 and one with an unbounded decision tree. You can use the sklearn predict_proba function to provide a probability distribution over the class values, but create your own ROC curve rather than using the sklearn roc_curve function. Ideally you would generate the ROC curve on a holdout subset of data, but for simplicity in this case you can build it using the entire dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miRcygK2Ee4j",
        "colab_type": "text"
      },
      "source": [
        "#Answers\n",
        "#1\n",
        "If you raise the classification threshold for a ML algorithm that determines whether emails are \"junk\" or \"legitimate\", then the number of emails that will be flagged as \"junk\" will decrease and the number of emails flagged as \"legitimate\" will increase. This means that the False Positive (FP) and False Negative (FN) values will change as well. Since we will be raising the threshold, our FN number will increase and the number of FP will decrease. <br><br>Thus, because Precision = TP/(TP+FP) and Recall = TP/(TP+FN), then changing our threshold will result in the Precision of our alg to decrease and the Recall of our alg to increase.\n",
        "\n",
        "---\n",
        "#2\n",
        "For classification 2 involving the \"Dragon Pox\", the disease affect 1 out of every 10,000 people (0.01% of population). So if a classifier was to test all 10,000 people, and just classify them all to be healthy, it would result in an accuracy of 9,999/10,000 = 99.99% which is exactly what the classifier did. So by doing nothing and guessing that everyone is healthy you could be just as accurate as C2 was. C1 on the other hand would have a chance to randomly guess the slot the ball lands on with an accuracy of 1/38 = 2.6%. This algorithm is able to achive an accuracy of 4%, so I think that C1 is doing a better job than C2 is in this case because C2 adds almost nothing of value.\n",
        "\n",
        "---\n",
        "#3\n",
        "Quicksort has a runtime of O(Nlog_2(N)). Since this RankTest function uses the function f as the comparison function, it will have a runtime similar to quicksort but it depends on f. Thus, the runtime is O(Nlog_2(N)) calls to f.\n",
        "\n",
        "---\n",
        "#4\n",
        "To calculate the t value in a paired t-test, we need to get the sum of the differences of the two pairs. \n",
        "<br>\n",
        "The average of group A is .6155<br>\n",
        "The average of group B is .6455<br>\n",
        "The average difference of the groups is A-B which is .6155 - .6455 = -.0291 which we will call x_d <br>\n",
        "The standard deviation of the differences is .06204 and we will call it s_d <br>\n",
        "The number of entries we have is n=11.\n",
        "<br>\n",
        "The formula for t is t=(x_d)/(s_d/sqrt(n)) which we can fill out our variables and come up with t=1.555 <br>\n",
        "Now, looking at our t-table and using a 95% confidence interval and 10 degrees of freedom, we can clearly see that this t value is less than 2.228 and within our range. Therefore, we can conclude that the differences between A and B are **not statistically significant**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pphNw2FnEcrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "15809510-9f3e-406c-daaa-68e807179e40"
      },
      "source": [
        "# QUESTION 5\n",
        "import numpy as np\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = np.loadtxt(fname='/content/alldata.csv', delimiter=',')\n",
        "\n",
        "def func_accuracy(ML_model, X, y):\n",
        "  Precision = 0.0\n",
        "  Recall = 0.0\n",
        "  TP = 0\n",
        "  FP = 0\n",
        "  FN = 0\n",
        "  element = ML_model.predict(X)\n",
        "  for i in range(0, element.shape[0]):\n",
        "    if ((element[i] == 1) and (y[i] == 1)):\n",
        "      TP += 1\n",
        "    elif ((element[i] == 0) and (y[i] == 1)):\n",
        "      FN += 1\n",
        "    elif ((element[i] == 1) and (y[i] == 0)):\n",
        "      FP += 1\n",
        "  Precision = TP/(TP+FP)\n",
        "  Recall = TP/(TP+FN)\n",
        "  return Precision, Recall\n",
        "\n",
        "def func_macro(Precision, Recall):\n",
        "  Macro = 0.0\n",
        "  Macro = (2*Precision*Recall)/(Precision + Recall)\n",
        "  return Macro\n",
        "\n",
        "def func_print_results(pd, rd, pt, rt, num):\n",
        "  print()\n",
        "  print(\"Printing K-fold Number \", num)\n",
        "  print()\n",
        "  print(\"Precision for Dummy Classifier: \", pd)\n",
        "  print(\"Recall for Dummy Classifier: \", rd)\n",
        "  print(\"Precision for Tree: \", pt)\n",
        "  print(\"Recall for Tree: \", rt)\n",
        "\n",
        "#splitting the data up into 3 different pieces for the 3-fold\n",
        "temp = np.array_split(data, 3)\n",
        "\n",
        "#FIRST K FOLD\n",
        "data1 = temp[0]\n",
        "X1 = data1[:,:-1]\n",
        "y1 = data1[:,-1]\n",
        "d1 = DummyClassifier(strategy=\"stratified\")\n",
        "d1.fit(X1, y1)\n",
        "p_d1, r_d1 = func_accuracy(d1, X1, y1)\n",
        "md1 = func_macro(p_d1, r_d1)\n",
        "\n",
        "clf1 = DecisionTreeClassifier(random_state=0)\n",
        "clf1.fit(X1, y1)\n",
        "p_t1, r_t1 = func_accuracy(clf1, X1, y1)\n",
        "mt1 = func_macro(p_t1, r_t1)\n",
        "func_print_results(p_d1, r_d1, p_t1, r_t1, 1)\n",
        "\n",
        "\n",
        "#SECOND K FOLD\n",
        "data2 = temp[1]\n",
        "X2 = data2[:,:-1]\n",
        "y2 = data2[:,-1]\n",
        "d2 = DummyClassifier(strategy=\"stratified\")\n",
        "d2.fit(X2, y2)\n",
        "p_d2, r_d2 = func_accuracy(d2, X2, y2)\n",
        "md2 = func_macro(p_d2, r_d2)\n",
        "\n",
        "clf2 = DecisionTreeClassifier(random_state=0)\n",
        "clf2.fit(X2, y2)\n",
        "p_t2, r_t2 = func_accuracy(clf2, X2, y2)\n",
        "mt2 = func_macro(p_t2, r_t2)\n",
        "func_print_results(p_d2, r_d2, p_t2, r_t2, 2)\n",
        "\n",
        "\n",
        "#THIRD K FOLD\n",
        "data3 = temp[2]\n",
        "X3 = data3[:,:-1]\n",
        "y3 = data3[:,-1]\n",
        "d3 = DummyClassifier(strategy=\"stratified\")\n",
        "d3.fit(X3, y3)\n",
        "p_d3, r_d3 = func_accuracy(d3, X3, y3)\n",
        "md3 = func_macro(p_d3, r_d3)\n",
        "\n",
        "clf3 = DecisionTreeClassifier(random_state=0)\n",
        "clf3.fit(X3, y3)\n",
        "p_t3, r_t3 = func_accuracy(clf3, X3, y3)\n",
        "mt3 = func_macro(p_t3, r_t3)\n",
        "func_print_results(p_d3, r_d3, p_t3, r_t3, 3)\n",
        "\n",
        "\n",
        "#printing out all macros and average macro for each \n",
        "print()\n",
        "print(\"All 3 Dummy Classifier Macros: \", md1, \" \", md2, \" \", md3)\n",
        "MD = (md1 + md2 + md3)/3\n",
        "print(\"Average = \", MD)\n",
        "print(\"All 3 Tree Macros: \", mt1, \" \", mt2, \" \", mt3)\n",
        "MT = (mt1 + mt2 + mt3)/3\n",
        "print(\"Average = \", MT)\n",
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Printing K-fold Number  1\n",
            "\n",
            "Precision for Dummy Classifier:  0.490625\n",
            "Recall for Dummy Classifier:  0.5277310924369748\n",
            "Precision for Tree:  1.0\n",
            "Recall for Tree:  1.0\n",
            "\n",
            "Printing K-fold Number  2\n",
            "\n",
            "Precision for Dummy Classifier:  0.46422018348623856\n",
            "Recall for Dummy Classifier:  0.46938775510204084\n",
            "Precision for Tree:  1.0\n",
            "Recall for Tree:  1.0\n",
            "\n",
            "Printing K-fold Number  3\n",
            "\n",
            "Precision for Dummy Classifier:  0.506108202443281\n",
            "Recall for Dummy Classifier:  0.4931972789115646\n",
            "Precision for Tree:  1.0\n",
            "Recall for Tree:  1.0\n",
            "\n",
            "All Dummy Classifier Macros:  0.5085020242914979   0.466789667896679   0.49956933677863913\n",
            "Average =  0.4916203429889387\n",
            "All Tree Macros:  1.0   1.0   1.0\n",
            "Average =  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VypaVCf5K9SF",
        "colab_type": "text"
      },
      "source": [
        "#5 Continued\n",
        "\n",
        "The dummy classifier clearly does a much worse job because it does not do a great job predicting the outcomes, that is why it is a dummy classifier. The Tree actually does a fantastic job in predicting the data, with its precision scores almost always either 1.0 or .999 thus it is extremely accurate. Because my Tree classifier was usually so good, it was hard to see the difference between the two measures of Precision and Recall, but when looking at the Dummy Classifier we can see a much larger difference. This is because the Precision metric is meant to show the significance of the false positive (FP) much more and the Recall metric is used more for drawing attention to the number of false negatives (FN)"
      ]
    }
  ]
}
